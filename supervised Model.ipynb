{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(0.6417704011065007, 0.3370165745856354)"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Regression Model\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the newly uploaded file containing labeled training data\n",
    "file_path_with_labels = \"HAO_comment_classification.json\"\n",
    "data_with_labels = pd.read_json(file_path_with_labels)\n",
    "\n",
    "# Display the first few rows of the dataset to understand its structure\n",
    "data_with_labels.head()\n",
    "\n",
    "# Get the features and labels\n",
    "X = data_with_labels[\"Comment Body\"]\n",
    "y = data_with_labels[\"Comment Classification\"]\n",
    "\n",
    "# Split the data into training and test sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the size of the training and test sets\n",
    "X_train.shape, X_test.shape\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the TF-IDF features on the training set\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test set using the same vectorizer\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Display the shape of the transformed features\n",
    "X_train_tfidf.shape, X_test_tfidf.shape\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "logreg_model = LogisticRegression(max_iter=10000, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "logreg_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on the training and test sets\n",
    "y_train_pred = logreg_model.predict(X_train_tfidf)\n",
    "y_test_pred = logreg_model.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate accuracy on the training and test sets\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Display the accuracy on the training and test sets\n",
    "train_accuracy, test_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "(0.9930843706777317, 0.35359116022099446)"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest Model\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load the newly uploaded file containing labeled training data\n",
    "file_path_with_labels = \"HAO_comment_classification.json\"\n",
    "data_with_labels = pd.read_json(file_path_with_labels)\n",
    "\n",
    "# Display the first few rows of the dataset to understand its structure\n",
    "data_with_labels.head()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Get the features and labels\n",
    "X = data_with_labels[\"Comment Body\"]\n",
    "y = data_with_labels[\"Comment Classification\"]\n",
    "\n",
    "# Split the data into training and test sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the size of the training and test sets\n",
    "X_train.shape, X_test.shape\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the TF-IDF features on the training set\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test set using the same vectorizer\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Display the shape of the transformed features\n",
    "X_train_tfidf.shape, X_test_tfidf.shape\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "random_forest_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "random_forest_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on the training and test sets\n",
    "y_train_rf_pred = random_forest_model.predict(X_train_tfidf)\n",
    "y_test_rf_pred = random_forest_model.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate accuracy on the training and test sets\n",
    "from sklearn.metrics import accuracy_score # Note: Include this import at the top of the code\n",
    "train_rf_accuracy = accuracy_score(y_train, y_train_rf_pred)\n",
    "test_rf_accuracy = accuracy_score(y_test, y_test_rf_pred)\n",
    "\n",
    "train_rf_accuracy, test_rf_accuracy\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "(0.8796680497925311, 0.30386740331491713)"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Support Vector Machine (SVM) Model\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Load the newly uploaded file containing labeled training data\n",
    "file_path_with_labels = \"HAO_comment_classification.json\"\n",
    "data_with_labels = pd.read_json(file_path_with_labels)\n",
    "\n",
    "# Display the first few rows of the dataset to understand its structure\n",
    "data_with_labels.head()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Get the features and labels\n",
    "X = data_with_labels[\"Comment Body\"]\n",
    "y = data_with_labels[\"Comment Classification\"]\n",
    "\n",
    "# Split the data into training and test sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the size of the training and test sets\n",
    "X_train.shape, X_test.shape\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the TF-IDF features on the training set\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test set using the same vectorizer\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Display the shape of the transformed features\n",
    "X_train_tfidf.shape, X_test_tfidf.shape\n",
    "\n",
    "# Initialize the Support Vector Machine model\n",
    "svm_model = SVC(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "svm_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on the training and test sets\n",
    "y_train_svm_pred = svm_model.predict(X_train_tfidf)\n",
    "y_test_svm_pred = svm_model.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate accuracy on the training and test sets\n",
    "from sklearn.metrics import accuracy_score # Note: This import was missing in the original code\n",
    "train_svm_accuracy = accuracy_score(y_train, y_train_svm_pred)\n",
    "test_svm_accuracy = accuracy_score(y_test, y_test_svm_pred)\n",
    "\n",
    "train_svm_accuracy, test_svm_accuracy\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "18/18 [==============================] - 1s 21ms/step - loss: 3.4963 - accuracy: 0.1206 - val_loss: 3.4488 - val_accuracy: 0.1958\n",
      "Epoch 2/20\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 3.3386 - accuracy: 0.3462 - val_loss: 3.3096 - val_accuracy: 0.1818\n",
      "Epoch 3/20\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 3.1005 - accuracy: 0.3776 - val_loss: 3.1240 - val_accuracy: 0.2098\n",
      "Epoch 4/20\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 2.8224 - accuracy: 0.4108 - val_loss: 2.9867 - val_accuracy: 0.2448\n",
      "Epoch 5/20\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 2.5839 - accuracy: 0.4336 - val_loss: 2.9080 - val_accuracy: 0.2867\n",
      "Epoch 6/20\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 2.3314 - accuracy: 0.4773 - val_loss: 2.8358 - val_accuracy: 0.3007\n",
      "Epoch 7/20\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 2.0767 - accuracy: 0.5577 - val_loss: 2.7601 - val_accuracy: 0.3077\n",
      "Epoch 8/20\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 1.7940 - accuracy: 0.6836 - val_loss: 2.6838 - val_accuracy: 0.3077\n",
      "Epoch 9/20\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 1.5239 - accuracy: 0.7657 - val_loss: 2.6090 - val_accuracy: 0.3007\n",
      "Epoch 10/20\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 1.2761 - accuracy: 0.8164 - val_loss: 2.5404 - val_accuracy: 0.3427\n",
      "Epoch 11/20\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 1.0602 - accuracy: 0.8549 - val_loss: 2.4841 - val_accuracy: 0.3497\n",
      "Epoch 12/20\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.8673 - accuracy: 0.9056 - val_loss: 2.4338 - val_accuracy: 0.3497\n",
      "Epoch 13/20\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.7110 - accuracy: 0.9371 - val_loss: 2.3957 - val_accuracy: 0.3427\n",
      "Epoch 14/20\n",
      "18/18 [==============================] - 0s 13ms/step - loss: 0.5793 - accuracy: 0.9528 - val_loss: 2.3616 - val_accuracy: 0.3357\n",
      "Epoch 15/20\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.4868 - accuracy: 0.9650 - val_loss: 2.3354 - val_accuracy: 0.3776\n",
      "Epoch 16/20\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.4059 - accuracy: 0.9790 - val_loss: 2.3118 - val_accuracy: 0.3846\n",
      "Epoch 17/20\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.3457 - accuracy: 0.9790 - val_loss: 2.2957 - val_accuracy: 0.3846\n",
      "Epoch 18/20\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.2995 - accuracy: 0.9860 - val_loss: 2.2837 - val_accuracy: 0.3916\n",
      "Epoch 19/20\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 0.2554 - accuracy: 0.9825 - val_loss: 2.2773 - val_accuracy: 0.3846\n",
      "Epoch 20/20\n",
      "18/18 [==============================] - 0s 12ms/step - loss: 0.2269 - accuracy: 0.9895 - val_loss: 2.2732 - val_accuracy: 0.3986\n"
     ]
    },
    {
     "data": {
      "text/plain": "(0.9895104765892029, 0.3986014127731323)"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# neural network model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Reading the data\n",
    "file_path_with_labels = \"HAO_comment_classification.json\"\n",
    "data_with_labels = pd.read_json(file_path_with_labels)\n",
    "\n",
    "# Filtering out classes with only one sample\n",
    "class_counts = data_with_labels[\"Comment Classification\"].value_counts()\n",
    "single_sample_classes = class_counts[class_counts == 1].index\n",
    "data_with_labels_filtered = data_with_labels[~data_with_labels[\"Comment Classification\"].isin(single_sample_classes)]\n",
    "\n",
    "# Getting the features and labels\n",
    "X = data_with_labels_filtered[\"Comment Body\"]\n",
    "y = data_with_labels_filtered[\"Comment Classification\"]\n",
    "\n",
    "# Encoding the labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Splitting the data\n",
    "X_temp, X_test, y_temp_encoded, y_test_encoded = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
    "X_train, X_val, y_train_encoded, y_val_encoded = train_test_split(X_temp, y_temp_encoded, test_size=0.2, random_state=42, stratify=y_temp_encoded)\n",
    "\n",
    "# Vectorizing the features using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train).toarray()\n",
    "X_val_tfidf = tfidf_vectorizer.transform(X_val).toarray()\n",
    "\n",
    "# Converting integer labels to one-hot encoding\n",
    "y_train_one_hot = to_categorical(y_train_encoded)\n",
    "y_val_one_hot = to_categorical(y_val_encoded)\n",
    "\n",
    "# Defining the custom learning rate\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Defining the optimizer with the custom learning rate\n",
    "optimizer_with_custom_lr = Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Defining the neural network model with the previous configuration\n",
    "dropout_rate = 0.3\n",
    "model_with_custom_lr = Sequential([\n",
    "    Dense(256, input_shape=(X_train_tfidf.shape[1],), activation='relu', kernel_regularizer=l2(0.00001)),\n",
    "    Dropout(dropout_rate),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compiling the model with the custom optimizer\n",
    "model_with_custom_lr.compile(optimizer=optimizer_with_custom_lr, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "history_with_custom_lr = model_with_custom_lr.fit(X_train_tfidf, y_train_one_hot, epochs=20, batch_size=32, validation_data=(X_val_tfidf, y_val_one_hot), verbose=1)\n",
    "\n",
    "# Evaluating the model\n",
    "train_accuracy_with_custom_lr = model_with_custom_lr.evaluate(X_train_tfidf, y_train_one_hot, verbose=0)[1]\n",
    "val_accuracy_with_custom_lr = model_with_custom_lr.evaluate(X_val_tfidf, y_val_one_hot, verbose=0)[1]\n",
    "\n",
    "train_accuracy_with_custom_lr, val_accuracy_with_custom_lr\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
